{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python /home/uz1/projects/ImageNet-Datasets-Downloader/downloader.py \\\n",
    "#     -data_root /home/uz1/data \\\n",
    "#     -number_of_classes 10 \\\n",
    "#     -images_per_class 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset, GNNBenchmarkDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv as GCNConv, dense_diff_pool,DynamicEdgeConv\n",
    "\n",
    "max_nodes = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL as pl\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from skimage import future\n",
    "from torch_scatter import scatter_min\n",
    "\n",
    "\n",
    "class ImgPixelsToGraph(BaseTransform):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, img) -> Data:\n",
    "        # print(img.shape)\n",
    "        c, h, w = img.shape\n",
    "        img = img.reshape(h * w, -1)\n",
    "        edge_index, pos = grid(h//2, w//2)\n",
    "        adj = to_dense_adj(edge_index).squeeze()\n",
    "        return Data(img, adj=adj, edge_index=edge_index, pos=pos)\n",
    "\n",
    "\n",
    "class ImgToGraph(BaseTransform):\n",
    "    r\"\"\"Converts an image to a superpixel representation using the\n",
    "    :meth:`skimage.segmentation.slic` algorithm, resulting in a\n",
    "    :obj:`torch_geometric.data.Data` object holding the centroids of\n",
    "    superpixels in :obj:`pos` and their mean color in :obj:`x`\n",
    "    (functional name: :obj:`to_slic`).\n",
    "\n",
    "    This transform can be used with any :obj:`torchvision` dataset.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        from torchvision.datasets import MNIST\n",
    "        import torchvision.transforms as T\n",
    "        from torch_geometric.transforms import ToSLIC\n",
    "\n",
    "        transform = T.Compose([T.ToTensor(), ToSLIC(n_segments=75)])\n",
    "        dataset = MNIST('/tmp/MNIST', download=True, transform=transform)\n",
    "\n",
    "    Args:\n",
    "        add_seg (bool, optional): If set to `True`, will add the segmentation\n",
    "            result to the data object. (default: :obj:`False`)\n",
    "        add_img (bool, optional): If set to `True`, will add the input image\n",
    "            to the data object. (default: :obj:`False`)\n",
    "        **kwargs (optional): Arguments to adjust the output of the SLIC\n",
    "            algorithm. See the `SLIC documentation\n",
    "            <https://scikit-image.org/docs/dev/api/skimage.segmentation.html\n",
    "            #skimage.segmentation.slic>`_ for an overview.\n",
    "    \"\"\"\n",
    "    def __init__(self, add_seg=False, add_img=False, **kwargs):\n",
    "        self.add_seg = add_seg\n",
    "        self.add_img = add_img\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __call__(self, img, mask, n_seg=250):\n",
    "        segments_slic = segmentation.slic(img,\n",
    "                                          n_segments=n_seg,\n",
    "                                          compactness=10,\n",
    "                                          sigma=1,\n",
    "                                          start_label=0)\n",
    "\n",
    "        seg = torch.from_numpy(segments_slic)\n",
    "        rag = rag_mean_band(img[:, :, :],\n",
    "                            segments_slic,\n",
    "                            connectivity=2,\n",
    "                            mode='similarity',\n",
    "                            sigma=255.0,\n",
    "                            ch=img.shape[2])\n",
    "\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        mask[mask != 0] = 1\n",
    "        mask = torch.from_numpy(mask)[:, :, :1]\n",
    "        h, w, c = img.shape\n",
    "        # pinta ll shapes\n",
    "        #   print(seg.shape,img.shape,mask.shape)\n",
    "        x = scatter_mean(img.view(h * w, c), seg.view(h * w), dim=0)\n",
    "\n",
    "        pos_y = torch.arange(h, dtype=torch.float)\n",
    "        pos_y = pos_y.view(-1, 1).repeat(1, w).view(h * w)\n",
    "        pos_x = torch.arange(w, dtype=torch.float)\n",
    "        pos_x = pos_x.view(1, -1).repeat(h, 1).view(h * w)\n",
    "\n",
    "        pos = torch.stack([pos_x, pos_y], dim=-1)\n",
    "        pos = scatter_mean(pos, seg.view(h * w), dim=0)\n",
    "\n",
    "        edge_index = np.asarray([[n1, n2] for (n1, n2) in rag.edges\n",
    "                                 ]).reshape(2, -1)  #connectivity coodinates\n",
    "        weights = np.asarray([w[2]['weight'] for w in rag.edges.data()])\n",
    "        x = np.asarray([n[1]['mean color'] for n in rag.nodes.items()])\n",
    "        node_class = scatter_min(mask.view(h * w), seg.reshape(h * w),\n",
    "                                 dim=0)[0]\n",
    "\n",
    "        #   lc = future.graph.show_rag(seg, rag, img[:,:,:3])\n",
    "\n",
    "        #   pos= np.asarray([n[1]['centroid'] for n in rag.nodes.items()])\n",
    "\n",
    "        data = Data(x=torch.from_numpy(x),\n",
    "                    pos=pos,\n",
    "                    edge_index=torch.tensor(edge_index),\n",
    "                    edge_weight=torch.tensor(weights).unsqueeze(1),\n",
    "                    y=node_class[:])\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uz1/miniconda3/envs/ML/lib/python3.7/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DenseDataLoader' is deprecated, use 'loader.DenseDataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from sklearn.utils import shuffle\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch_geometric.utils import grid, to_dense_adj\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "class ImageFolderGraph(datasets.ImageFolder):\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 transform:  None,\n",
    "                 ):\n",
    "        super().__init__(root, transform,)\n",
    "\n",
    "        # self.samples = shuffle(imagenet.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) ->  Data:\n",
    "        data, y = super().__getitem__(index)\n",
    "        data.y = y\n",
    "        return data\n",
    "\n",
    "\n",
    "transfrom = T.Compose([T.Resize((64, 64)), T.ToTensor(), ImgPixelsToGraph()])\n",
    "imagenet = ImageFolderGraph(\"/home/uz1/data/imagenet_images\",\n",
    "                                transform=transfrom)\n",
    "n = (len(imagenet) ) // 10\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "y= [y[1] for y in imagenet.samples]\n",
    "for train_index, test_index in sss.split(imagenet,y):\n",
    "\n",
    "# test_loader = DenseDataLoader(test_dataset, batch_size=8)\n",
    "# val_loader = DenseDataLoader(val_dataset, batch_size=8)\n",
    "# train_loader = DenseDataLoader(train_dataset, batch_size=8)\n",
    "\n",
    "    val_dataset = SubsetRandomSampler(test_index)\n",
    "# test_dataset = SubsetRandomSampler(test_dataset)\n",
    "    train_dataset = SubsetRandomSampler(train_index)\n",
    "train_loader = DenseDataLoader(imagenet, batch_size=16,sampler=train_dataset)\n",
    "# test_loader= DenseDataLoader(imagenet, batch_size=16,sampler=test_dataset)\n",
    "val_loader = DenseDataLoader(imagenet, batch_size=16,sampler=val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 100, 1600)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader),len(train_loader),len(train_dataset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet.num_features = imagenet[0].x.shape[1]\n",
    "imagenet.num_nodes = imagenet[0].x.shape[0]\n",
    "imagenet.num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet.num_features=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = imagenet\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 hidden_channels,\n",
    "                 out_channels,\n",
    "                 normalize=False,\n",
    "                 lin=True):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels, normalize))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.convs.append(GCNConv(hidden_channels, hidden_channels, normalize))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels, normalize))\n",
    "        self.bns.append(torch.nn.BatchNorm1d(out_channels))\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        batch_size, num_nodes, in_channels = x.size()\n",
    "\n",
    "        for step in range(len(self.convs)):\n",
    "            x = F.relu(self.convs[step](x, adj, mask))\n",
    "            # print(\"in frwd \", x.shape)\n",
    "            x = self.bns[step](x.permute(0, 2, 1))\n",
    "            # print(\"after bn\",x.shape)\n",
    "            x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DGNN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 hidden_channels,\n",
    "                 out_channels,\n",
    "                 normalize='batch',\n",
    "                 lin=True):\n",
    "        super(DGNN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(EdgeConv2d(in_channels, hidden_channels, norm=normalize))\n",
    "\n",
    "        self.convs.append(EdgeConv2d(hidden_channels, hidden_channels,norm=normalize))\n",
    "\n",
    "        self.convs.append(EdgeConv2d(hidden_channels, out_channels, norm=normalize))\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        # batch_size, num_nodes, in_channels = x.size()\n",
    "\n",
    "        for step in range(len(self.convs)):\n",
    "            x = F.relu(self.convs[step](x, adj, mask))\n",
    "            # print(\"in frwd \", x.shape)\n",
    "            # x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiffPool, self).__init__()\n",
    "\n",
    "        num_nodes = ceil(0.25 * max_nodes)\n",
    "        self.gnn1_pool = GNN(dataset.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = GNN(dataset.num_features, 64, 64)\n",
    "\n",
    "        num_nodes = ceil(0.25 * num_nodes)\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = GNN(64, 64, 64, lin=False)\n",
    "\n",
    "        self.gnn3_embed = GNN(64, 64, 64, lin=False)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(64, 64)\n",
    "        self.lin2 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "        self.stem = Stem(64,3,128)\n",
    "\n",
    "    def forward(self, x, adj, mask=None,return_clusters=False):\n",
    "        x_temp = x\n",
    "        #add stem downsampling\n",
    "        b,n,f = x.shape \n",
    "        x = self.stem(x.view(b,3,int(n**.5),-1))\n",
    "        x_stem=x\n",
    "        x=x.reshape(b,-1,128)\n",
    "        # print(x.shape,adj.shape)\n",
    "        s1 = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s1, mask)\n",
    "        #x_1 = s_0.t() @ z_0\n",
    "        #adj_1 = s_0.t() @ adj_0 @ s_0\n",
    "\n",
    "        s2 = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s2)\n",
    "\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        if return_clusters : return x_stem, x_temp, s1,s2\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "\n",
    "\n",
    "\n",
    "class DynDiffPool(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DynDiffPool, self).__init__()\n",
    "\n",
    "        num_nodes = ceil(0.25 * max_nodes)\n",
    "        self.gnn1_pool = DGNN(dataset.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DGNN(dataset.num_features, 64, 64)\n",
    "\n",
    "        num_nodes = ceil(0.25 * num_nodes)\n",
    "        self.gnn2_pool = DGNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DGNN(64, 64, 64, lin=False)\n",
    "\n",
    "        self.gnn3_embed = DGNN(64, 64, 64, lin=False)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(64, 64)\n",
    "        self.lin2 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "        self.stem = Stem(64,3,128)\n",
    "\n",
    "    def forward(self, x, adj, mask=None,return_clusters=False):\n",
    "        x_temp = x\n",
    "        #add stem downsampling\n",
    "        b,n,f = x.shape \n",
    "        x = self.stem(x.view(b,3,int(n**.5),-1))\n",
    "        x_stem=x\n",
    "        x=x.reshape(b,128,-1,1)\n",
    "        ad = dense_knn_matrix(x,16)\n",
    "        # print(x.shape,adj.shape)\n",
    "        s1 = self.gnn1_pool(x, ad, mask)\n",
    "        x = self.gnn1_embed(x, ad, mask)\n",
    "\n",
    "        x, adj, l1, e1 = dense_diff_pool(x.reshape(b,-1,64), adj, s1.reshape(b,n,-1), mask)\n",
    "        #x_1 = s_0.t() @ z_0\n",
    "        #adj_1 = s_0.t() @ adj_0 @ s_0\n",
    "\n",
    "        x=x.reshape(b,128,-1,1)\n",
    "        ad = dense_knn_matrix(x,16)\n",
    "\n",
    "        s2 = self.gnn2_pool(x, ad)\n",
    "        x = self.gnn2_embed(x, ad)\n",
    "\n",
    "\n",
    "        x, adj, l2, e2 = dense_diff_pool(x.reshape(b,-1,64), adj, s2.reshape(b,n,-1), mask)\n",
    "\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        if return_clusters : return x_stem, x_temp, s1,s2\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "import torch.nn as nn\n",
    "def act_layer(act, inplace=False, neg_slope=0.2, n_prelu=1):\n",
    "    # activation layer\n",
    "\n",
    "    act = act.lower()\n",
    "    if act == 'relu':\n",
    "        layer = nn.ReLU(inplace)\n",
    "    elif act == 'leakyrelu':\n",
    "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
    "    elif act == 'prelu':\n",
    "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
    "    elif act == 'gelu':\n",
    "        layer = nn.GELU()\n",
    "    elif act == 'hswish':\n",
    "        layer = nn.Hardswish(inplace)\n",
    "    else:\n",
    "        raise NotImplementedError('activation layer [%s] is not found' % act)\n",
    "    return layer\n",
    "\n",
    "class Stem(nn.Module):\n",
    "    \"\"\" Image to Visual Embedding\n",
    "    Overlap: https://arxiv.org/pdf/2106.13797.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, in_dim=3, out_dim=768, act='relu'):\n",
    "        super().__init__()        \n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            act_layer(act),\n",
    "            nn.Conv2d(out_dim, out_dim, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = DiffPool().to(device)\n",
    "# model = torch.load(\"/home/uz1/projects/GCN/checkpoint\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "losses = AverageMeter()\n",
    "val_accc = AverageMeter()\n",
    "test_accc = AverageMeter()\n",
    "'''\n",
    "- stem downsamples from 64x64 to 16x16 - adj matches that\n",
    "- num features per node is 128 - output of stem \n",
    "'''\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for i,data in enumerate(tqdm(train_loader,total=len(train_loader))):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(data)\n",
    "        output, _, _ = model(data.x, data.adj)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        losses.update(loss)\n",
    "        loss_all += data.y.size(0) * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in tqdm(loader,total=len(loader)):\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.adj)[0].max(dim=1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "        # print(correct,len(loader.sampler.indices))\n",
    "    return correct / len(loader.sampler.indices)\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 151):\n",
    "    train_loss = train(epoch)\n",
    "    val_acc = test(val_loader)\n",
    "    val_accc.update(val_acc)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "    print(\n",
    "        f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f} ({losses.avg}), '\n",
    "        f'Val Acc: {val_acc:.4f} ({val_accc.avg})'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"/home/uz1/projects/GCN/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/home/uz1/projects/GCN/checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffPool().to(device)\n",
    "@torch.no_grad()\n",
    "def test_get_clusters(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    s1s=[]\n",
    "    s2s=[]\n",
    "    xs=[]\n",
    "    preds=[]\n",
    "    for data in tqdm(loader,total=len(loader)):\n",
    "        data = data.to(device)\n",
    "        pred,x,s1,s2 = model(data.x, data.adj,return_clusters=True)\n",
    "        s1s.append(s1)\n",
    "        s2s.append(s2)\n",
    "        xs.append(x)\n",
    "        preds.append(pred)\n",
    "    return s1s,s2s,xs,preds\n",
    "s1s,s2s,xs,preds = test_get_clusters(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[1][1].mean(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL as pl \n",
    "def show_img(x,pil=False):\n",
    "    if len(x.shape) > 2:\n",
    "        c,n,w = x.shape\n",
    "        x = x.mean(0)\n",
    "        c=1\n",
    "    else:\n",
    "        n,c=x.shape\n",
    "        \n",
    "        n=int(n**.5)\n",
    "    if pil:\n",
    "        img = x.reshape(c,n,-1)\n",
    "        to_img = T.ToPILImage()\n",
    "        img = to_img(img)\n",
    "        return img\n",
    "    img = x.reshape(n,-1,c)\n",
    "    return img.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s[0][0].argmax(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import graph, data, io, segmentation, color\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "\n",
    "def get_img_labels(s1s,s2s,xs,preds):\n",
    "    for i,(s1,s2,x,pred) in enumerate(zip(s1s,s2s,xs,preds)):\n",
    "        # print(x)\n",
    "        img = show_img(x,True)\n",
    "        \n",
    "        pred = show_img(pred) \n",
    "\n",
    "        plt.subplot(1,3,1,)\n",
    "        plt.imshow(pred)\n",
    "    \n",
    "\n",
    "        plt.subplot(1,3,2,)\n",
    "        plt.imshow(img)\n",
    "        label_shape = int(s1.shape[0] ** .5) \n",
    "        labels = s1.argmax(1).reshape(label_shape,label_shape).cpu().detach().numpy()\n",
    "\n",
    "        label_rgb = color.label2rgb(labels, np.asarray(img), kind='avg')\n",
    "        regions = regionprops(labels)\n",
    "        label_img = pl.Image.fromarray(label_rgb)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(label_img)\n",
    "\n",
    "        plt.figure(figsize = (50,50)) \n",
    "        # print(s1.shape,s2.shape,x.shape)\n",
    "get_img_labels(s1s[15],s2s[15],xs[15],preds[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1s[0][4].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[1].mean(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding K-means to the Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdding K means to the mix \\n-Find the patchs place in the space \\n-Cluster based on embed \\ncraete graph \\nvis? - when before after \\nclassify ? \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Adding K means to the mix \n",
    "-Find the patchs place in the space \n",
    "-Cluster based on embed \n",
    "craete graph \n",
    "vis? - when before after \n",
    "classify ? \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "# x_em = x.reshape(16,-1,128)\n",
    "# KMeans(n_clusters=7).fit(x_em[0].cpu().detach().numpy()).labels_.shape\n",
    "'''\n",
    "A\n",
    "'''\n",
    "\n",
    "def pairwise_distance(x):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x_inner = -2*torch.matmul(x, x.transpose(2, 1))\n",
    "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
    "        return x_square + x_inner + x_square.transpose(2, 1)\n",
    "\n",
    "\n",
    "def part_pairwise_distance(x, start_idx=0, end_idx=1):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x_part = x[:, start_idx:end_idx]\n",
    "        x_square_part = torch.sum(torch.mul(x_part, x_part), dim=-1, keepdim=True)\n",
    "        x_inner = -2*torch.matmul(x_part, x.transpose(2, 1))\n",
    "        x_square = torch.sum(torch.mul(x, x), dim=-1, keepdim=True)\n",
    "        return x_square_part + x_inner + x_square.transpose(2, 1)\n",
    "def dense_knn_matrix(x, k=16, relative_pos=None):\n",
    "    \"\"\"Get KNN based on the pairwise distance.\n",
    "    Args:\n",
    "        x: (batch_size, num_dims, num_points, 1)\n",
    "        k: int\n",
    "    Returns:\n",
    "        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = x.transpose(2, 1).squeeze(-1)\n",
    "        batch_size, n_points, n_dims = x.shape\n",
    "        ### memory efficient implementation ###\n",
    "        n_part = 10000\n",
    "        if n_points > n_part:\n",
    "            nn_idx_list = []\n",
    "            groups = math.ceil(n_points / n_part)\n",
    "            for i in range(groups):\n",
    "                start_idx = n_part * i\n",
    "                end_idx = min(n_points, n_part * (i + 1))\n",
    "                dist = part_pairwise_distance(x.detach(), start_idx, end_idx)\n",
    "                if relative_pos is not None:\n",
    "                    dist += relative_pos[:, start_idx:end_idx]\n",
    "                _, nn_idx_part = torch.topk(-dist, k=k)\n",
    "                nn_idx_list += [nn_idx_part]\n",
    "            nn_idx = torch.cat(nn_idx_list, dim=1)\n",
    "        else:\n",
    "            dist = pairwise_distance(x.detach())\n",
    "            if relative_pos is not None:\n",
    "                dist += relative_pos\n",
    "            _, nn_idx = torch.topk(-dist, k=k) # b, n, k\n",
    "        ######\n",
    "        center_idx = torch.arange(0, n_points, device=x.device).repeat(batch_size, k, 1).transpose(2, 1)\n",
    "    return torch.stack((nn_idx, center_idx), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, Conv2d\n",
    "def act_layer(act, inplace=False, neg_slope=0.2, n_prelu=1):\n",
    "    # activation layer\n",
    "\n",
    "    act = act.lower()\n",
    "    if act == 'relu':\n",
    "        layer = nn.ReLU(inplace)\n",
    "    elif act == 'leakyrelu':\n",
    "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
    "    elif act == 'prelu':\n",
    "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
    "    elif act == 'gelu':\n",
    "        layer = nn.GELU()\n",
    "    elif act == 'hswish':\n",
    "        layer = nn.Hardswish(inplace)\n",
    "    else:\n",
    "        raise NotImplementedError('activation layer [%s] is not found' % act)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def norm_layer(norm, nc):\n",
    "    # normalization layer 2d\n",
    "    norm = norm.lower()\n",
    "    if norm == 'batch':\n",
    "        layer = nn.BatchNorm2d(nc, affine=True)\n",
    "    elif norm == 'instance':\n",
    "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm)\n",
    "    return layer\n",
    "class BasicConv(Seq):\n",
    "    def __init__(self, channels, act='relu', norm=None, bias=True, drop=0.):\n",
    "        m = []\n",
    "        for i in range(1, len(channels)):\n",
    "            m.append(Conv2d(channels[i - 1], channels[i], 1, bias=bias, groups=4))\n",
    "            if norm is not None and norm.lower() != 'none':\n",
    "                m.append(norm_layer(norm, channels[-1]))\n",
    "            if act is not None and act.lower() != 'none':\n",
    "                m.append(act_layer(act))\n",
    "            if drop > 0:\n",
    "                m.append(nn.Dropout2d(drop))\n",
    "\n",
    "        super(BasicConv, self).__init__(*m)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "def batched_index_select(x, idx):\n",
    "    r\"\"\"fetches neighbors features from a given neighbor idx\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): input feature Tensor\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times 1}`.\n",
    "        idx (Tensor): edge_idx\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times l}`.\n",
    "    Returns:\n",
    "        Tensor: output neighbors features\n",
    "            :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times C \\times N \\times k}`.\n",
    "    \"\"\"\n",
    "    batch_size, num_dims, num_vertices_reduced = x.shape[:3]\n",
    "    _, num_vertices, k = idx.shape\n",
    "    idx_base = torch.arange(0, batch_size, device=idx.device).view(-1, 1, 1) * num_vertices_reduced\n",
    "    idx = idx + idx_base\n",
    "    idx = idx.contiguous().view(-1)\n",
    "\n",
    "    x = x.transpose(2, 1)\n",
    "    feature = x.contiguous().view(batch_size * num_vertices_reduced, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_vertices, k, num_dims).permute(0, 3, 1, 2).contiguous()\n",
    "    return feature\n",
    "\n",
    "class EdgeConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Edge convolution layer (with activation, batch normalization) for dense data type\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):\n",
    "        super(EdgeConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels * 2, out_channels], act, norm, bias)\n",
    "\n",
    "    def forward(self, x, edge_index, y=None):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        max_value, _ = torch.max(self.nn(torch.cat([x_i, x_j - x_i], dim=1)), -1, keepdim=True)\n",
    "        return max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DGNN(\n",
       "  (convs): ModuleList(\n",
       "    (0): EdgeConv2d(\n",
       "      (nn): BasicConv(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): EdgeConv2d(\n",
       "      (nn): BasicConv(\n",
       "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): EdgeConv2d(\n",
       "      (nn): BasicConv(\n",
       "        (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bns): ModuleList()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DGNN(128,64,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bf7c8448272c4cbdce3f78384e0b31dc492bbd9290e96311fca142ad432e9ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
